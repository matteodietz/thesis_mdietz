\chapter{Introduction}
\label{ch:introduction}

For decades, medical ultrasound has been a cornerstone of diagnostics imaging with broad clinical and preclinical applications, valued for its real-time capabilities, non-ionizing nature and relatively low cost.\cite{CompressedDrori2021} Continuous innovation, from new imaging and processing techniques to portable hardware and sophisticated software, ensures its central role in fields including oncology, cardiology, brain imaging, obstretics\cite{DualNyayapathi2024}, emergency medicine and many more. As technology advances, the demand for higher resolution and more sophisticated imaging modes constantly pushes system designers to capture more data with an ever-increasing number of channels and higher sampling rates. The management of these massive datastreams strains the limits of acquisition, real-time processing and transmission in ultrasound-based imaging modalities. Addressing this data-rate bottle neck is therefore a critical step in enabling future innovations.

Data-rate reduction, respectively data compression in general can be approached from a variety of different angles. We can broadly categorise them in two classes: \textbf{Lossy} and \textbf{lossless} compression.

Prior work on lossy methods has shown that very high compression ratios can be achieved by transforming the ultrasound signal into another domain and intelligently discarding information that is deemed less important, respectively considered to be noise. For instance, Drori et al.\cite{CompressDrori2021} have proposed a novel beamforming algorithm in the frequency domain that is equivalent to time domain DAS. This, combined with compressed sensing techniques and sub-Nyquist sampling allows them to filter and massively compress the ultrasound data by exploiting the finite rate of innovation (FRI) structure of the beamformed data.

Similar techniques based on the Discrete Wavelet Transform (DWT) can efficiently denoise and significantly reduce the dimensionality of medical imaging data by thresholding small wavelet coefficients. In the landscape of lossy compression algorithms, the DWT shows distinguished potential due to the following characteristics: It contains information about the signal in time and frequency domain simultaneously and it is able to solve the three problems noisiness, excessive storage space and low information content.\cite{RNS-BasedNagornov2022} These properties make it a promising paradigm for further work on resource constrained systems.

While these lossy techniques are powerful and represent a compelling field of research, they are fundamentally ill-suited for the objectives of a high-performance, flexible research platform like the \textit{ListenToLight} system used in this thesis. This platform is characterised by its high-bandwidth architecture, capable of transferring 80Gbit/s to the host PC. The primary goal of such a system is to provide highest possible quality data without compromising signal fidelity. In the context of \textit{ListenToLight}, three key issues make aggressive, lossy compression undesirable.

\begin{enumerate}
  \item Unacceptable Loss of Fidelity: Any lossy step at the acquisition stage can permanently degrade the ground truth data, which defies the purpose of a high-end research system. 
  \item Violation of the Open Platform Philosophy: Approaches like Frequency Doimain Beamforming force the downstream user into a specific reconstruction method, restricting the freedom of researchers to develop and test their own novel algorithms in software.
  \item Excessive Compression: With the ability to handle massive data rates, the significant trade-off in signal quality for a >4x compression factor is difficult to motivate.
\end{enumerate}

Given these constraints, the focus must therefore shift to lossless data-rate reduction methods. The principle behind a viable lossless approach lies in removing redundancy, not information. 

\dots transducer specific frequency behaviour allows for higher decimation ratios \dots

depth dependent frequency attenuation -> signal's bw not const. etc.


% However, the \textit{ListenToLight} research platform, which is being used in the scope of this thesis, has very high bandwidth architecture. Hence, this system is not required to perform such aggressive compressed sensing directly on the FPGA. It should rather provide the highest quality data possible and not compromise signal fidelity. This requirement rules out the inherently lossy data reduction techniques mentioned above. In this sense, we should shift our focus to lossless data-rate reduction methods.

% intro to listentolight taken from Milos, need to reformulate.

%\textit{ListenToLight} aims to provide a flexible and open-source hardware-software platform for optoacoustic and ultrasound modalities, with a focus on medical applications. The platform is based on the AMD-Xilinx Zynq UltraScale+ XCZU19EG FPGA, which is well suited to handle large volumes of data generated by modern ultrasound systems. \textit{ListenToLight} has two operating modes: ultra-fast raw data transfer, and a slower real-time image preview mode for clinical use. This thesis aims to expand on the real-time image preview capabilities, which come with some issues. The sheer amount of data makes real-time processing harder, given resource and throughput constraints. At the same time, a lack of data processing and quality assessment is not suitable for real-time feedback. One must strike a balance between on-board processing for real-time feedback and offloading more compute-intensive processes to external systems.


%Furthermore, since ListenToLight is a research platform, the type of reconstruction should not be restricted as for example in the frequency domain beamforming approach mentioned earlier.

This thesis proposes the design and implementation of such a lossless method: A real-time, closed-loop Adaptive Decimation Filter. The main goal of the system is to intelligently adapt the acquisition hardware to the real-time characteristics of the signal, controlling the decimation factor of the Analog Front End (AFE) and low-pass filter (LPF) parameters to match the signal's measured bandwidth.

The proposed architecture consists of three main stages implemented on the FPGA:

\begin{itemize}
  \item Sensing: A real-time Short Time Fourier Transform (STFT) processor continuously measures the instantanous bandwidth of the incoming I/Q data.
  \item Control: A decision logic block determines the maximum safe decimation factor that upholds the Nyquist-Shannon Sampling Theorem.
  \item Actuation \& Packaging: A control module reprograms the AFE's hardware decimator in real-time, while the resulting variable-rate datastream is packaged for the host PC.
\end{itemize}

This approach provides several key benefits. It is lossless by design, directly meets the system's bandwidth constraints, and is robust and flexible, able to work with various transducers with different frequency behaviour. It represents a contribution to the system-level design of the \textit{ListenToLight} platform by providing a hardware-integrated solution for intelligent data management.


% put QASA as future work in section 6 makes more sense this way


% from compression frequency domain bf paper:
%s that of the data-rate bottleneck in US imaging. The formation of a US image requires large amounts of data due mostly to two factors: High sampling rates, multiples or even magnitudes of order beyond the Nyquist rate to ensure high resolution beamforming, and a large number of data channels (and corresponding receivers), typically tens to hundreds, to enable high spatial resolution. These high data rates constrain advanced processing algorithms and portable hardware